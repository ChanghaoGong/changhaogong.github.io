---
layout: post
title:  "你是否在为老板让你归纳每月领域内的新文献发愁？"
date:  2023-01-18 16:48:30       
categories: python
tags: 文献 爬虫
---


* content
{:toc}

读文献是个苦力活，但是筛选到自己需要的文献再精度可以省很多事，那么有没有什么办法可以定制一个领域内最新文献的列表，提取出主Figure、翻译好Abstract，到时间就推送到你眼前？仔细想想，上面这些活都是重复性的工作，完全可以用脚本解决，那就开干。

# 1. 使用RSS订阅获取文献推送
大多数文献网站都有RSS订阅功能，其实完全可以用邮箱或者zotero这种文献管理软件获取RSS推送的文章。以[pubmed高级搜索](https://pubmed.ncbi.nlm.nih.gov/advanced/)为例，比如你想看单细胞和空间转录组最新的Science和Nature上的文章，可以使用

```
((Nature[Journal])) OR (Science[Journal]) AND ((single cell) OR (spatial transcriptome))
```

但是每个文章还要点开看，像我这种英语不大行的还要翻译一下才舒服，其实还是不大方便。

# 2. python解析RSS订阅链接，总结成markdown

```python
import feedparser
import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime

# 替换为你的PubMed RSS订阅链接
rss_url = 'https://pubmed.ncbi.nlm.nih.gov/rss/search/xxx'
# od by time
cur_time = time.strftime("%Y%m%d", time.localtime())
# 获取当前日期和时间
current_date = datetime.now()
# 输出文件名
od = f'Sci_Nat_{cur_time}.md'
# 解析XML
feed = feedparser.parse(rss_url)
## 提取文章信息到markdown，不带图片版本地辣鸡翻译版
# 创建一个Markdown字符串来保存信息
import argostranslate.package
import argostranslate.translate

markdown_content = ""

# 遍历每个文章项
for entry in feed.entries:
    # 提取文章标题
    title = entry.title
    
    # 提取作者列表
    if 'authors' not in entry.keys():
        continue
    authors = ", ".join(author.name for author in entry.authors)
    
    # 提取发布日期
    published_date_str = entry.published
    published_date_obj = datetime.strptime(published_date_str, "%a, %d %b %Y %H:%M:%S %z")
    # 只提取当月的文章
    if published_date_obj.year != current_date.year or published_date_obj.month != current_date.month:
        continue
    published_date = published_date_obj.strftime("%Y-%m-%d")
    
    # 提取文章简介
    content = entry.content[0].value
    summary = re.search(r'<p><b>ABSTRACT</b></p><p>(.+?)</p>', content).group(1)
    # 翻译成中文
    summary_zh = argostranslate.translate.translate(summary, 'en', 'zh')
    
    # 提取杂志名称
    journal_name = entry.dc_source
    
    # 提取DOI信息
    doi = entry.dc_identifier
    doi = doi.replace('doi:', '')
    
    article_url = f'https://doi.org/{doi}'
    image_url = None
                
    # 构建Markdown条目，包括图片
    entry_markdown = f'''# <span style="color: red;">{journal_name}</span>\n\n''' \
                    f"## {title}\n\n" \
                    f"**作者:** {authors}\n\n" \
                    f"**发布日期:** {published_date}\n\n" \
                    f"**Abstract:** {summary}\n\n" \
                    f"**简介:** {summary_zh}\n\n" \
                    f"**DOI:** [{doi}]({article_url})\n\n" \
                    f'''<div style="text-align: center"><img src={image_url} width=70% /></div>\n\n'''

    # 将Markdown条目添加到总内容中
    markdown_content += entry_markdown

# 保存Markdown内容到文件
with open(od, 'w', encoding='utf-8') as md_file:
    md_file.write(markdown_content)

print(f"Markdown内容已保存到{od}文件。")
```

这样就获得了当月最新的文献以及它的垃圾机翻，但是，都什么年代了还在用传统机翻？可以试试免费的翻译api。


# 3. 提取nature和pmc的图片链接
爬图片这个事，因为文献网站构造不一样，目前我只写了pmc和nature系列的。大家可以自由发挥。
```python
# 提取nature和pmc的图片链接
def parse_nature_images(url):
    response = requests.get(url)
    image_list = []
    if response.status_code == 200:
        # 使用BeautifulSoup解析网页内容
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 查找所有img标签，并筛选出src属性中含有‘Fig*_HTML.png’的图片
        pattern = re.compile(r'Fig\d+_HTML.png')
        images = soup.find_all('img', {'src': pattern})
        
        # 提取图片的URL信息并存储到列表中
        image_urls = [img['src'] for img in images]

        # 打印图片URL列表
        for img_url in image_urls:
            image_list.append(f'https:{img_url}')
            # print(f'https:{img_url}')
        return image_list
    else:
        return 0
    

def parse_pmc_images(url):
    cookies = {
        'F12抄一个cookie'
    }

    headers = {
        'authority': 'www.ncbi.nlm.nih.gov',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7,ja;q=0.6',
        'sec-ch-ua': '"Chromium";v="118", "Google Chrome";v="118", "Not=A?Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'none',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
    }

    response = requests.get(url, cookies=cookies, headers=headers)
    if response.status_code == 200:
        # 使用BeautifulSoup解析网页内容
        soup = BeautifulSoup(response.text, 'html.parser')
        image_urls = []
        # 查找所有img标签，并筛选出alt属性以"figure"开头的图片
        divs = soup.find_all('div', class_='figure')
        for div in divs:
            img_link = div.find('img')['src']
            image_urls.append(f'https://www.ncbi.nlm.nih.gov{img_link}')
            # print(f'https://www.ncbi.nlm.nih.gov{img_link}')
        return image_urls
    else:
        return 0
```
# 4. 百度翻译api
老实说，百度的不大好用，但起码能看懂了，有时间改成google的。。。
```python
# baidu translate api
import httpx
import hashlib

def get_baidu_api_credentials():
    try:
        with open(os.path.expanduser('~/.baiduapi'), 'r') as file:
            lines = file.readlines()
            appid = lines[0].strip()
            apikey = lines[1].strip()
            return appid, apikey
    except FileNotFoundError:
        print("Error: ~/.baiduapi file not found. Please regist at https://fanyi-api.baidu.com/ and write to ~/.baiduapi")
        return None, None
    
def translate(text: str, lang_from: str = "auto", lang_to: str = "zh") -> str:
    appid, apikey = get_baidu_api_credentials()
    if not appid or not apikey:
        return "Error: Baidu API credentials not found. Please regist at https://fanyi-api.baidu.com/ and write to ~/.baiduapi"
    salt = str(round(time.time() * 1000))
    appid = appid
    apikey = apikey
    domain = 'senimed'
    sign_raw = appid + text + salt + apikey
    sign = hashlib.md5(sign_raw.encode("utf8")).hexdigest()
    params = {
        "q": text,
        "from": lang_from,
        "to": lang_to,
        "appid": appid,
        "salt": salt,
        "domain": domain,
        "sign": sign,
    }
    url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    # 其实百度有专业领域翻译api，但是收费，所以先凑合用。。。
    with httpx.Client() as client:
        resp = client.get(url, params=params)
        result = resp.json()
        print(result)
    return result["trans_result"][0]["dst"]
```

# 5. 爬取并生成md
```python
## 提取文章信息到markdown，同时爬取图片链接并采用baidu api翻译

# 创建一个Markdown字符串来保存信息
markdown_content = ""

# 遍历每个文章项
for entry in feed.entries:
    # 提取文章标题
    title = entry.title
    
    # 提取作者列表
    if 'authors' not in entry.keys():
        continue
    authors = ", ".join(author.name for author in entry.authors)
    
    # 提取发布日期
    published_date_str = entry.published
    published_date_obj = datetime.strptime(published_date_str, "%a, %d %b %Y %H:%M:%S %z")
     # 只提取当月的文章，此处根据需求改成每周也行
    if published_date_obj.year != current_date.year or published_date_obj.month != current_date.month:
        continue
    published_date = published_date_obj.strftime("%Y-%m-%d")
    
    
    # 提取文章简介
    content = entry.content[0].value
    summary = re.search(r'<p><b>ABSTRACT</b></p><p>(.+?)</p>', content).group(1)
    # 翻译成中文
    # summary_zh = argostranslate.translate.translate(summary, 'en', 'zh')
    summary_zh = translate(summary, lang_to="zh")
    
    # 提取杂志名称
    journal_name = entry.dc_source
    
    # 提取DOI信息
    doi = entry.dc_identifier
    doi = doi.replace('doi:', '')
    
    # 爬取文章页面以查找图片
    image_block = ''
    article_url = f'https://doi.org/{doi}'
    if journal_name == 'Nature immunology':
        image_list = parse_nature_images(article_url)
        if len(image_list) > 0:
            for image_url in image_list[:3]:
                image_block += f'''<div style="text-align: center"><img src={image_url} width="312" /></div>\n\n'''
        else:
            image_block = f'''<div style="text-align: center"><img src=None width=70% /></div>\n\n'''
            
    elif journal_name == 'Science advances':
        # 使用正则表达式提取URL
        url_pattern = r'href="(https://www.ncbi.nlm.nih.gov/pmc/PMC\d+).*"'
        match = re.search(url_pattern, content)
        image_block = ''
        if match:
            extracted_url = match.group(1)
            image_list = parse_pmc_images(extracted_url)
            if len(image_list) > 0:
                for image_url in image_list[:3]:
                    image_block += f'''<div style="text-align: center"><img src={image_url} width=70% /></div>\n\n'''
        if len(image_block) == 0:
            image_block = f'''<div style="text-align: center"><img src=None width=70% /></div>\n\n'''
                
    # 构建Markdown条目，包括图片
    entry_markdown = f'''# <span style="color: red;">{journal_name}</span>\n\n''' \
                    f"## {title}\n\n" \
                    f"**作者:** {authors}\n\n" \
                    f"**发布日期:** {published_date}\n\n" \
                    f"**Abstract:** {summary}\n\n" \
                    f"**简介:** {summary_zh}\n\n" \
                    f"**DOI:** [{doi}]({article_url})\n\n" \
                    f'{image_block}'

    # 将Markdown条目添加到总内容中
    markdown_content += entry_markdown

# 保存Markdown内容到文件
with open(od, 'w', encoding='utf-8') as md_file:
    md_file.write(markdown_content)

print(f"Markdown内容已保存到{od}文件。")
```

**大功告成！可以扔服务器上挂定时任务每周运行一次，email给你自己，直接少了好多工作量！**